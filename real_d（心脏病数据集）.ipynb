{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11bc9048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import sqrt, e, log\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.font_manager\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e9328ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifierWithWeight:\n",
    "    def __init__(self):\n",
    "        self.best_err = 1  # 最小的加权错误率 \n",
    "        self.best_fea_id = 0  # 最优特征id\n",
    "        self.best_thres = 0  # 选定特征的最优阈值\n",
    "        self.best_op = 1  # 阈值符号，其中 1: >, 0: < \n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        if sample_weight is None:\n",
    "            sample_weight = np.ones(len(X)) / len(X)\n",
    "        n = X.shape[1]\n",
    "        for i in range(n):\n",
    "            feature = X[:, i]  # 选定特征列\n",
    "            fea_unique = np.sort(np.unique(feature))  # 将所有特征值从小到大排序\n",
    "            for j in range(len(fea_unique)-1):\n",
    "                thres = (fea_unique[j] + fea_unique[j+1]) / 2  # 逐一设定可能阈值\n",
    "                for op in (0, 1):\n",
    "                    y_ = 2*(feature >= thres)-1 if op==1 else 2*(feature < thres)-1  # 判断何种符号为最优\n",
    "                    err = np.sum((y_ != y)*sample_weight)\n",
    "                    if err < self.best_err:  # 当前参数组合可以获得更低错误率，更新最优参数\n",
    "                        self.best_err = err\n",
    "                        self.best_op = op\n",
    "                        self.best_fea_id = i\n",
    "                        self.best_thres = thres\n",
    "        return self\n",
    "       \n",
    "    def predict(self, X):\n",
    "        feature = X[:, self.best_fea_id]\n",
    "        return 2*(feature >= self.best_thres)-1 if self.best_op==1 else 2*(feature < self.best_thres)-1\n",
    "\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        y_pre = self.predict(X)\n",
    "        if sample_weight is not None:\n",
    "            return np.sum((y_pre == y)*sample_weight) \n",
    "        return np.mean(y_pre == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e69b7980",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _AdaBoostClassifier:\n",
    "    def __init__(self, n_estimators=50):\n",
    "        self.n_estimators = n_estimators \n",
    "        self.estimators = []\n",
    "        self.alphas = []\n",
    "        self.betas = []\n",
    "        self.k = 5 # k neighbors\n",
    "        self.d = 1 # dimension\n",
    "        self.train_PCA = np.zeros((self.d,))\n",
    "        \n",
    "        self.test_PCA = np.empty((0,) * self.d)\n",
    "#—————————自己加的函数   计算社会程度  cal_similarity  ——————\n",
    "    def cal_similarity(self, X, y):\n",
    "        pca = PCA(n_components = 2)  # 将数据降至2维\n",
    "        X_pca = pca.fit_transform(X)\n",
    "        print(\"降维完成\")\n",
    "        k = self.k  # 设置K值为5\n",
    "        nbrs = NearestNeighbors(n_neighbors=k, algorithm='auto').fit(X_pca)\n",
    "        # 查询每个点的最近的K个邻居\n",
    "        distances, indices = nbrs.kneighbors(X_pca)\n",
    "        # print(\"每个点的最近K个邻居的索引：\", indices)\n",
    "        # 计算每个点的邻居中与自身标签相同的比例y\n",
    "        matching_ratios = []\n",
    "        for i in range(len(X_pca)):\n",
    "            neighbors = y[indices[i][1:]]  # 排除自身，获取邻居的标签\n",
    "            matching_ratio = np.mean(neighbors == y[i])  # 计算相同标签的比例\n",
    "            matching_ratios.append(matching_ratio)\n",
    "        #print(\"每个点的邻居中与自身标签相同的比例：\", matching_ratios)\n",
    "        return matching_ratios\n",
    "#—————————自己加的函数   计算辅助函数  cal_d  ——————\n",
    "\n",
    "    \n",
    "    def cal_d(self, s, pred_y, c, PCA_x):\n",
    "\n",
    "        k = self.k\n",
    "        # 找到类别c里离s最近的k个点的索引\n",
    "        indices = np.where(pred_y == c)[0]\n",
    "        # 使用'cityblock'距离度量，即曼哈顿距离\n",
    "        distances = cdist(PCA_x[s][0].reshape(-1, 1), PCA_x[indices][0].reshape(-1, 1), 'cityblock')  \n",
    "        # 找到距离最近的k个点\n",
    "        nearest_indices = indices[np.argsort(distances.flatten())[:k]]\n",
    "        # 计算这k个点到s的距离的均值\n",
    "        mean_distance = np.mean(np.abs(PCA_x[nearest_indices] - PCA_x[s]))\n",
    "        return mean_distance\n",
    "    \n",
    "    def cal_deviation(self, s, pred_y, c, X):\n",
    "        pca = PCA(n_components = 2)  # 将数据降至2维\n",
    "        PCA_x = pca.fit_transform(X)\n",
    "        k = self.k\n",
    "        # 找到类别c里离s最近的k个点\n",
    "        indices = np.where(pred_y == c)[0]\n",
    "        distances = cdist(PCA_x[s][0].reshape(-1, 1), PCA_x[indices][0].reshape(-1, 1), 'cityblock')  \n",
    "        neighbors_index = indices[np.argsort(distances.flatten())[:k]]\n",
    "        deviation = np.exp( - self.cal_d(s, pred_y, c, PCA_x) / np.mean([self.cal_d(i, pred_y, c, PCA_x) for i in neighbors_index]))\n",
    "\n",
    "        return deviation\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        pca = PCA(n_components = self.d)\n",
    "        X_pca = pca.fit_transform(X).flatten()\n",
    "        self.train_PCA = X_pca\n",
    "\n",
    "        #print(self.train_PCA,\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "        sample_weight = np.ones(len(X)) / len(X)  # 初始化样本权重为 1/N\n",
    "        for _ in range(self.n_estimators):\n",
    "            dtc = DecisionTreeClassifierWithWeight().fit(X, y, sample_weight)  # 训练弱学习器\n",
    "            alpha = 1/2 * np.log((1-dtc.best_err)/dtc.best_err)  #权重系数alpha\n",
    "            y_pred = dtc.predict(X)\n",
    "            \n",
    "            print(\"_______________________分界线______________________\")\n",
    "            print(X)\n",
    "            print(y_pred)\n",
    "            print(alpha)\n",
    "           \n",
    "            if _>40:\n",
    "                self.cal_similarity(X, y_pred)\n",
    "                for i in range(1,50):\n",
    "                    print(\"______i______\")\n",
    "                    print(self.cal_deviation(i, y_pred, 1, X))\n",
    "                    print(self.cal_deviation(i, y_pred, -1, X))\n",
    "            \n",
    "            \n",
    "            \n",
    "            sample_weight *= np.exp(-alpha * y_pred * y)  # 更新迭代样本权重\n",
    "            sample_weight /= np.sum(sample_weight)  # 样本权重归一化\n",
    "            self.estimators.append(dtc)\n",
    "            self.alphas.append(alpha)\n",
    "            # ------------------权重系数bata--------------------\n",
    "            beta = 0\n",
    "            self.betas.append(beta)\n",
    "        return self   \n",
    "   \n",
    "    def predict(self, X):\n",
    "        y_pred = np.empty((len(X), self.n_estimators))  # 预测结果二维数组，其中每一列代表一个弱学习器的预测结果\n",
    "        for i in range(self.n_estimators):\n",
    "            y_pred[:, i] = self.estimators[i].predict(X)\n",
    "        y_pred = y_pred * np.array(self.alphas)  # 将预测结果与训练权重乘积作为集成预测结果\n",
    "        return 2 * (np.sum(y_pred, axis=1) > 0) - 1  # 以0为阈值，判断并映射为-1和1\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred==y)\n",
    "    \n",
    "    #—————————自己加的函数——————\n",
    "    #def cal_similarity(center_point，Neighbors)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d917addd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'HeartDiseaseorAttack'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'HeartDiseaseorAttack'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m      6\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataslice.csv\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m----> 7\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHeartDiseaseorAttack\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHeartDiseaseorAttack\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mint\u001b[39m(x))\n\u001b[0;32m      8\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHeartDiseaseorAttack\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHeartDiseaseorAttack\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(convert_zero)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'HeartDiseaseorAttack'"
     ]
    }
   ],
   "source": [
    "def convert_zero(x):\n",
    "    if x == 0:\n",
    "        x = -1\n",
    "    return x\n",
    "\n",
    "df = pd.read_csv('dataslice.csv') \n",
    "df['HeartDiseaseorAttack'] = df['HeartDiseaseorAttack'].apply(lambda x: int(x))\n",
    "df['HeartDiseaseorAttack'] = df['HeartDiseaseorAttack'].apply(convert_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c78e821",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045dee3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['HeartDiseaseorAttack'])\n",
    "y = df['HeartDiseaseorAttack']\n",
    "\n",
    "X_train, X_test, y_test, y_train = train_test_split(X, y, test_size = 0.5)\n",
    "#print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550c1865",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_AdaBoostClassifier().fit(X_train.values, y_train.values).score(X_test.values, y_test.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d05c406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn import metrics\n",
    "# model_rf = RandomForestClassifier(n_estimators=1000, oob_score=True, n_jobs=-1,\n",
    "#                                   random_state=50, max_features=None,\n",
    "#                                   max_leaf_nodes=30)\n",
    "# #print(X_train)\n",
    "# model_rf.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions\n",
    "# prediction_test = model_rf.predict(X_test)\n",
    "# print (metrics.accuracy_score(y_test, prediction_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a3c096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importances = model_rf.feature_importances_\n",
    "# weights = pd.Series(importances,\n",
    "#                  index=X_train.columns.values)\n",
    "# weights.sort_values()[-50:].plot(kind = 'barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adad4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = weights.sort_values(ascending=False)\n",
    "# columns = list(weights.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f90219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ca0e05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# error_bounds = []\n",
    "# errors_diff = []\n",
    "# exceptions = 0\n",
    "# indices = []\n",
    "\n",
    "\n",
    "# for i in tqdm(range(1, len(columns))):\n",
    "#     cols_to_use = columns[:i+1]\n",
    "#     df_exp = df[cols_to_use]\n",
    "#     df_exp['HeartDiseaseorAttack'] = df['HeartDiseaseorAttack']\n",
    "\n",
    "#     R_test, R_train, error_bound = evaluate_error(df_exp, 0.05)\n",
    "#     errors_diff.append(abs(R_test - R_train))\n",
    "#     error_bounds.append(error_bound)\n",
    "#     indices.append(i)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064ac1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_error_bounds_d = pd.DataFrame()\n",
    "# df_experiment_d = pd.DataFrame()\n",
    "\n",
    "# #error_bounds_exp = [i/len(error_bounds) for i in error_bounds]\n",
    "# df_error_bounds_d['d'] = indices\n",
    "# df_error_bounds_d['error'] = error_bounds\n",
    "# df_error_bounds_d['error'] = df_error_bounds_d['error'].apply(lambda x: x[0])\n",
    "\n",
    "# df_experiment_d['d'] = indices\n",
    "# df_experiment_d['error'] = errors_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9633035",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# error_bounds_sep = [get_error_bound(20, d, len(df)//2, 0.05) for d in tqdm(indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed91a4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bounds_d = pd.DataFrame()\n",
    "# df_bounds_d['bound'] = error_bounds_sep\n",
    "# df_bounds_d.to_csv('bounds_d.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0ca844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_experiment_d['bound'] = error_bounds_sep\n",
    "# df_experiment_d.to_csv('experiment_d.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582bd1d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "2a8dfe095fce2b5e88c64a2c3ee084c8e0e0d70b23e7b95b1cfb538be294c5c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
